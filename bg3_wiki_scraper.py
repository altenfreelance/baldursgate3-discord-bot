{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fab9ca-2012-4c5d-adb2-9942756ca4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://baldursgate3.wiki.fextralife.com/\"\n",
    "OUTPUT_FILE = \"/workspace/bg3_wiki_data.jsonl\"\n",
    "\n",
    "def get_links_from_page(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = set()\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if href.startswith(BASE_URL):\n",
    "                links.add(href)\n",
    "            elif href.startswith(\"/\") and not href.startswith(\"//\"):\n",
    "                links.add(BASE_URL + href.strip(\"/\"))\n",
    "        return list(links)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to get links from {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_page_text(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        content_div = soup.find(\"div\", {\"class\": \"wiki-content\"})\n",
    "        if content_div:\n",
    "            return content_div.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            print(f\"[!] No content div found at {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to scrape {url}: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "def crawl_wiki(base_url, max_pages=300, output_file=OUTPUT_FILE):\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        while to_visit and len(visited) < max_pages:\n",
    "            url = to_visit.pop(0)\n",
    "            if url in visited:\n",
    "                continue\n",
    "            print(f\"Scraping: {url}\")\n",
    "            content = get_page_text(url)\n",
    "            if content:\n",
    "                f.write(json.dumps({\"text\": content}) + \"\\n\")\n",
    "            visited.add(url)\n",
    "            links = get_links_from_page(url)\n",
    "            to_visit.extend([link for link in links if link not in visited and link not in to_visit])\n",
    "            time.sleep(1)  # polite crawl delay\n",
    "        print(f\"âœ… Saved {len(visited)} pages to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_wiki(BASE_URL, max_pages=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed768a-41bb-4cdb-a019-c28a429d6e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
